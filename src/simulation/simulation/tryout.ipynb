{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium stable-baselines3 shimmy gymnasium[box2d] tqdm"
      ],
      "metadata": {
        "id": "kakqJxh3gvDK"
      },
      "id": "kakqJxh3gvDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run `pip install \"gymnasium[classic-control]\"` for this example.\n",
        "import gymnasium as gym\n",
        "\n",
        "# Create our training environment - a cart with a pole that needs balancing\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "# Reset environment to start a new episode\n",
        "observation, info = env.reset()\n",
        "# observation: what the agent can \"see\" - cart position, velocity, pole angle, etc.\n",
        "# info: extra debugging information (usually not needed for basic learning)\n",
        "\n",
        "print(f\"Starting observation: {observation}\")\n",
        "# Example output: [ 0.01234567 -0.00987654  0.02345678  0.01456789]\n",
        "# [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
        "\n",
        "episode_over = False\n",
        "total_reward = 0\n",
        "\n",
        "while not episode_over:\n",
        "    # Choose an action: 0 = push cart left, 1 = push cart right\n",
        "    action = env.action_space.sample()  # Random action for now - real agents will be smarter!\n",
        "\n",
        "    # Take the action and see what happens\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # reward: +1 for each step the pole stays upright\n",
        "    # terminated: True if pole falls too far (agent failed)\n",
        "    # truncated: True if we hit the time limit (500 steps)\n",
        "\n",
        "    total_reward += reward\n",
        "    episode_over = terminated or truncated\n",
        "\n",
        "print(f\"Episode finished! Total reward: {total_reward}\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY98Rueegssz",
        "outputId": "0adffa47-b805-428f-8288-c0c133c85cff"
      },
      "id": "xY98Rueegssz",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting observation: [ 0.00613935  0.04930774  0.0210934  -0.00365053]\n",
            "Episode finished! Total reward: 24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import FlattenObservation\n",
        "\n",
        "# Start with a complex observation space\n",
        "env = gym.make(\"CarRacing-v3\")\n",
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90wNqLVxhLgu",
        "outputId": "53a89643-c468-4362-b5c4-f21df81f38e4"
      },
      "id": "90wNqLVxhLgu",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 96, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap it to flatten the observation into a 1D array\n",
        "wrapped_env = FlattenObservation(env)\n",
        "wrapped_env.observation_space.shape\n",
        "\n",
        "# This is useful for scenarios where some algorithms expect a 1D input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfWqk-EXhOLK",
        "outputId": "07cf86ed-3cae-4950-dbcf-79ab6bf70014"
      },
      "id": "PfWqk-EXhOLK",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27648,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Agent that actually learns stuff"
      ],
      "metadata": {
        "id": "6vvPLqO0jzJk"
      },
      "id": "6vvPLqO0jzJk"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class BlackJackAgent:\n",
        "\n",
        "\n",
        "  def __init__(\n",
        "        self,\n",
        "        env: gym.Env, # The environment\n",
        "        learning_rate: float, # How fast the agent updates the Q-value\n",
        "        initial_epsilon: float, # Starting exploration rate\n",
        "        epsilon_decay: float, # The rate at which the exploration rate reduces after each episode\n",
        "        final_epsilon: float, # Final exploration rate\n",
        "        discount_factor: float = 0.95, # Value of future rewards\n",
        "    ):\n",
        "\n",
        "    self.env = env\n",
        "\n",
        "    # Q-table: maps (state, action) to expected reward\n",
        "    # defaultdict automatically creates entries with zeros for new states\n",
        "    self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    self.lr = learning_rate\n",
        "    self.discount_factor = discount_factor  # How much we care about future rewards\n",
        "\n",
        "    # Exploration parameters\n",
        "    self.epsilon = initial_epsilon\n",
        "    self.epsilon_decay = epsilon_decay\n",
        "    self.final_epsilon = final_epsilon\n",
        "\n",
        "    # Track learning progress\n",
        "    self.training_error = []\n",
        "\n",
        "\n",
        "  def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
        "    \"\"\"\n",
        "    Returns 0 (stand) or 1 (hit)\n",
        "    \"\"\"\n",
        "    # with some probability epsilon, the agent should explore\n",
        "    if np.random.random() < self.epsilon:\n",
        "      return self.env.action_space.sample()\n",
        "    else:\n",
        "      return int(np.argmax(self.q_values[obs]))\n",
        "\n",
        "\n",
        "  def update(\n",
        "        self,\n",
        "        obs: tuple[int, int, bool],\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: tuple[int, int, bool],\n",
        "    ):\n",
        "        \"\"\"Update Q-value based on experience.\n",
        "\n",
        "        This is the heart of Q-learning: learn from (state, action, reward, next_state)\n",
        "        \"\"\"\n",
        "        # What's the best we could do from the next state?\n",
        "        # (Zero if episode terminated - no future rewards possible)\n",
        "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
        "\n",
        "        # What should the Q-value be? (Bellman equation)\n",
        "        target = reward + self.discount_factor * future_q_value\n",
        "\n",
        "        # How wrong was our current estimate?\n",
        "        temporal_difference = target - self.q_values[obs][action]\n",
        "\n",
        "        # Update our estimate in the direction of the error\n",
        "        # Learning rate controls how big steps we take\n",
        "        self.q_values[obs][action] = (\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "        )\n",
        "\n",
        "        # Track learning progress (useful for debugging)\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "\n",
        "  def decay_epsilon(self):\n",
        "      \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
        "      self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n"
      ],
      "metadata": {
        "id": "-3EbS748h89Y"
      },
      "id": "-3EbS748h89Y",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "learning_rate = 0.01        # How fast to learn (higher = faster but less stable)\n",
        "n_episodes = 100_000        # Number of hands to practice\n",
        "start_epsilon = 1.0         # Start with 100% random actions\n",
        "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
        "final_epsilon = 0.1         # Always keep some exploration\n",
        "\n",
        "# Create environment and agent\n",
        "env = gym.make(\"Blackjack-v1\", sab=False)\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
        "\n",
        "agent = BlackJackAgent(\n",
        "    env=env,\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        ")"
      ],
      "metadata": {
        "id": "Rbk91tYAnfrt"
      },
      "id": "Rbk91tYAnfrt",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "\n",
        "  obs, info = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "\n",
        "    action = agent.get_action(obs=obs)\n",
        "\n",
        "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    agent.update(obs=obs, action=action, next_obs=next_obs, reward=reward, terminated=terminated)\n",
        "\n",
        "    done = terminated or truncated\n",
        "    obs = next_obs\n",
        "\n",
        "  # Reduce exploration rate\n",
        "  agent.decay_epsilon()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Y8Wlu0np_A",
        "outputId": "c74aacb6-35e4-4d3f-905b-edfc9ef1d973"
      },
      "id": "h1Y8Wlu0np_A",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [00:17<00:00, 5625.12it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}